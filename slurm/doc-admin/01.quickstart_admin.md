# 管理员快速入门指南

## 概述

Please see the [Quick Start User Guide](https://slurm.schedmd.com/quickstart.html) for a general overview.

Also see [Platforms](https://slurm.schedmd.com/platforms.html) for a list of supported computer platforms.

This document also includes a section specifically describing how to perform [upgrades](https://slurm.schedmd.com/quickstart_admin.html#upgrade).

## 超级快速入门

1. Make sure the clocks, users and groups (UIDs and GIDs) are synchronized across the cluster.

2. Install [MUNGE](https://dun.github.io/munge/) for authentication.

   Make sure that all nodes in your cluster have the same munge.key.

   Make sure the MUNGE daemon, munged is started before you start the Slurm daemons.

3. bunzip2 the distributed tar-ball and untar the files:

   tar --bzip -x -f slurm*tar.bz2

4. cd to the directory containing the Slurm source and type ./configure with appropriate options, typically --prefix= and --sysconfdir=

5. Type make to compile Slurm.

6. Type make install to install the programs, documentation, libraries, header files, etc.

7. Build a configuration file using your favorite web browser and doc/html/configurator.html.

   NOTE: The SlurmUser must exist prior to starting Slurm and must exist on all nodes of the cluster.

   NOTE: The parent directories for Slurm's log files, process ID files, state save directories, etc. are not created by Slurm.

   They must be created and made writable by SlurmUser as needed prior to starting Slurm daemons.

   NOTE: If any parent directories are created during the installation process (for the executable files, libraries, etc.), those directories will have access rights equal to read/write/execute for everyone minus the umask value (e.g. umask=0022 generates directories with permissions of "drwxr-r-x" and mask=0000 generates directories with permissions of "drwxrwrwx" which is a security problem).

8. Type ldconfig -n <library_location> so that the Slurm libraries can be found by applications that intend to use Slurm APIs directly.

9. Install the configuration file in <sysconfdir>/slurm.conf.

   NOTE: You will need to install this configuration file on all nodes of the cluster.

10. systemd (optional): enable the appropriate services on each system:

   + Controller: **`systemctl enable slurmctld`**

   + Database: **`systemctl enable slurmdbd`**

   + Compute Nodes: **`systemctl enable slurmd`**

11. Start the slurmctld and slurmd daemons.

NOTE: Items 3 through 8 can be replaced with

1. **`rpmbuild -ta slurm*.tar.bz2`**

2. **`rpm --install <the rpm files>`**

FreeBSD administrators should see the [FreeBSD](https://slurm.schedmd.com/quickstart_admin.html#FreeBSD) section below.

## 构建和安装Slurm

Instructions to build and install Slurm manually are shown below.

See the README and INSTALL files in the source distribution for more details.

1. Unpack the distributed tarball:

   tar -xaf slurm*tar.bz2

2. cd to the directory containing the Slurm source and type ./configure with appropriate options (see below).

3. Type make install to compile and install the programs, documentation, libraries, header files, etc.

4. Type ldconfig -n <library_location> so that the Slurm libraries can be found by applications that intend to use Slurm APIs directly.

   The library location will be a subdirectory of PREFIX (described below) and depend upon the system type and configuration, typically lib or lib64.

   For example, if PREFIX is "/usr" and the subdirectory is "lib64" then you would find that a file named "/usr/lib64/libslurm.so" was installed and the command ldconfig -n /usr/lib64 should be executed.








### RPMs Installed

## Daemons

### High Availability

## Infrastructure

### User and Group Identification

### Authentication of Slurm communications

### MPI support

### Scheduler support

### Resource selection

### Logging

### Accounting

### Compute node access

## Configuration

## Security

### Authentication

### Pluggable Authentication Module (PAM) support

## Starting the Daemons

## 管理示例

scontrol can be used to print all system information and modify most of it.

Only a few examples are shown below.

Please see the scontrol man page for full details.

The commands and options are all case insensitive.

Print detailed state of all jobs in the system.

```shell
adev0: scontrol
scontrol: show job
JobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED
   Priority=4294901286 Partition=batch BatchFlag=0
   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED
   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59
   NodeList=adev8 NodeListIndecies=-1
   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0
   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0
   ReqNodeList=(null) ReqNodeListIndecies=-1

JobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING
   Priority=4294901285 Partition=batch BatchFlag=0
   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED
   StartTime=03/19-12:54:01 EndTime=NONE
   NodeList=adev8 NodeListIndecies=8,8,-1
   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0
   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0
   ReqNodeList=(null) ReqNodeListIndecies=-1
```

Print the detailed state of job 477 and change its priority to zero.

A priority of zero prevents a job from being initiated (it is held in "pending" state).

```shell
adev0: scontrol
scontrol: show job 477
JobId=477 UserId=bob(6885) Name=sleep JobState=PENDING
   Priority=4294901286 Partition=batch BatchFlag=0
   more data removed....
scontrol: update JobId=477 Priority=0
```

Print the state of node adev13 and drain it.

To drain a node, specify a new state of DRAIN, DRAINED, or DRAINING.

Slurm will automatically set it to the appropriate value of either DRAINING or DRAINED depending on whether the node is allocated or not.

Return it to service later.

```shell
adev0: scontrol
scontrol: show node adev13
NodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000
   Weight=16 Partition=debug Features=(null)
scontrol: update NodeName=adev13 State=DRAIN
scontrol: show node adev13
NodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000
   Weight=16 Partition=debug Features=(null)
scontrol: quit
Later
adev0: scontrol
scontrol: show node adev13
NodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000
   Weight=16 Partition=debug Features=(null)
scontrol: update NodeName=adev13 State=IDLE
```

Reconfigure all Slurm daemons on all nodes.

This should be done after changing the Slurm configuration file.

```shell
adev0: scontrol reconfig
```

Print the current Slurm configuration.

This also reports if the primary and secondary controllers (slurmctld daemons) are responding.

To just see the state of the controllers, use the command ping.

```shell
adev0: scontrol show config
Configuration data as of 2019-03-29T12:20:45
...
SlurmctldAddr           = eadevi
SlurmctldDebug          = info
SlurmctldHost[0]        = adevi
SlurmctldHost[1]        = adevj
SlurmctldLogFile        = /var/log/slurmctld.log
...

Slurmctld(primary) at adevi is UP
Slurmctld(backup) at adevj is UP
```

Shutdown all Slurm daemons on all nodes.

```shell
adev0: scontrol shutdown
```

## 升级

Background: The Slurm version number contains three period-separated numbers that represent both the major Slurm release and maintenance release level.

The first two parts combine together to represent the major release, and match the year and month of that major release.

The third number in the version designates a specific maintenance level: year.month.maintenance-release (e.g. 20.02.2 is major Slurm release 20.02, and maintenance version 2).

Thus version 20.02.x was initially released in February 2020.

Changes in the RPCs (remote procedure calls) and state files will only be made if the major release number changes, which typically happens about every nine months.

A list of most recent major Slurm releases is shown below.

+ 18.08.x (Released August 2018)
+ 19.05.x (Released May 2019)
+ 20.02.x (Released February 2020)

Slurm's MPI libraries may also change if the major release number change, requiring applications be re-linked (behavior may vary depending upon the MPI implementation used and the specific Slurm changes between releases).

Locally developed Slurm plugins may also require modification.

Slurm daemons will support RPCs and state files from the two previous major releases (e.g. a version 20.02.x SlurmDBD will support slurmctld daemons and commands with a version of 20.02.x, 19.05.x, or 18.08.x).

This means that upgrading at least once each year is recommended.

Otherwise, intermediate upgrades will be required to preserve state information.

Changes in the mainenance release number generally represent only bug fixes, but may also include minor enhancements.

If the SlurmDBD daemon is used, it must be at the same or higher major release number as the Slurmctld daemons.

In other words, when changing the version to a higher release number (e.g from 19.05.x to 20.02.x) always upgrade the SlurmDBD daemon first.

Database table changes may be required for the upgrade, for example adding new fields to existing tables.

If the database contains a large number of entries, the SlurmDBD daemon may require tens of minutes to update the database and be unresponsive during this time interval.

Before upgrading SlurmDBD it is strongly recommended to make a backup of the database.

When using mysqldump, the default behavior is to lock the tables, which can cause issues if SlurmDBD is still trying to send updates to the database.

To avoid locking the tables we recommend you use the --single-transaction flag with mysqldump.

This will dump a consistent state of the database without blocking any applications.

Note that in order for this flag to have the desired effect you must be using the InnoDB storage engine (specified by default when Slurm automatically creates any table).

The --single-transaction flag permits a live logical backup (and is thus useful for periodic backups while in production).

When upgrading Slurm, it is recommended to first stop the SlurmDBD daemon and then dump the database using mysqldump before proceeding with the upgrade, as stated in the upgrade guide a few paragraphs below.

Note that requests intended for SlurmDBD from Slurmctld will be queued while SlurmDBD is down, but the queue size is limited and you should monitor the DBD Agent Queue size with the sdiag command.

The slurmctld daemon must also be upgraded before or at the same time as the slurmd daemons on the compute nodes.

Generally, upgrading Slurm on all of the login and compute nodes is recommended, although rolling upgrades are also possible (i.e. upgrading the head node(s) first then upgrading the compute and login nodes later at various times).

Also see the note above about reverse compatibility.

Almost every new major release of Slurm (e.g. 19.05.x to 20.02.x) involves changes to the state files with new data structures, new options, etc.

Slurm permits upgrades to a new major release from the past two major releases, which happen every nine months (e.g. 18.08.x or 19.05.x to 20.02.x) without loss of jobs or other state information.

State information from older versions will not be recognized and will be discarded, resulting in loss of all running and pending jobs.

State files are not recognized when downgrading (e.g. from 19.05.x to 18.08.x) and will be discarded, resulting in loss of all running and pending jobs.

For this reason, creating backup copies of state files (as described below) can be of value.

Therefore when upgrading Slurm (more precisely, the slurmctld daemon), saving the StateSaveLocation (as defined in slurm.conf) directory contents with all state information is recommended.

If you need to downgrade, restoring that directory's contents will let you recover the jobs.

Jobs submitted under the new version will not be in those state files, but it can let you recover most jobs.

An exception to this is that jobs may be lost when installing new pre-release versions (e.g. 20.02.0-pre1 to 20.02.0-pre2).

Developers will try to note these cases in the NEWS file.

Contents of major releases are also described in the RELEASE_NOTES file.

The libslurm.so version is increased every major release.

So things like MPI libraries with Slurm integration should be recompiled.

Sometimes it works to just symlink the old .so name(s) to the new one, but this has no guarantee of working.

Be mindful of your configured SlurmdTimeout and SlurmctldTimeout values.

If the Slurm daemon's are down for longer than the specified timeout during an upgrade, nodes may be marked DOWN and their jobs killed.

You can either increase the timeout values during an upgrade or ensure that the slurmd daemons on compute nodes are not down for longer than SlurmdTimeout.

The recommended upgrade order is as follows:

1. Shutdown the slurmdbd daemon
2. Dump the Slurm database using mysqldump (in case of any possible failure) and increase innodb_buffer_size in my.cnf to 128M
3. Upgrade the slurmdbd daemon
4. Restart the slurmdbd daemon
5. Increase configured SlurmdTimeout and SlurmctldTimeout values and execute "scontrol reconfig" for them to take effect
6. Shutdown the slurmctld daemon(s)
7. Shutdown the slurmd daemons on the compute nodes
8. Copy the contents of the configured StateSaveLocation directory (in case of any possible failure)
9. Upgrade the slurmctld and slurmd daemons
10. Restart the slurmd daemons on the compute nodes
11. Restart the slurmctld daemon(s)
12. Validate proper operation
13. Restore configured SlurmdTimeout and SlurmctldTimeout values and execute "scontrol reconfig" for them to take effect
14. Destroy backup copies of database and/or state files

Note: It is possible to update the slurmd daemons on a node-by-node basis after the slurmctld daemon(s) are upgraded, but do make sure their down time is below the SlurmdTimeout value.

If you have built your own version of Slurm plugins, they will likely need modification to support a new version of Slurm.

It is common for plugins to add new functions and function arguments during major updates.

See the RELEASE_NOTES file for details.

## FreeBSD

FreeBSD administrators can install the latest stable Slurm as a binary package using:

```shell
pkg install slurm-wlm
```

Or, it can be built and installed from source using:

```shell
cd /usr/ports/sysutils/slurm-wlm && make install
```

The binary package installs a minimal Slurm configuration suitable for typical compute nodes.

Installing from source allows the user to enable options such as mysql and gui tools via a configuration menu.




